{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f09d95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP ELITE BOOK\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\HP ELITE BOOK\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP ELITE BOOK\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import urllib.parse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ddgs import DDGS\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae16374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SEARCH_RESULTS = 6\n",
    "PASSAGES_PER_PAGE = 4\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TOP_PASSAGES = 5\n",
    "SUMMARY_SENTENCES = 3\n",
    "TIMEOUT = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30057a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_ddg(url):\n",
    "    try:\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        if \"duckduckgo.com\" in parsed.netloc:\n",
    "            qs = urllib.parse.parse_qs(parsed.query)\n",
    "            uddg = qs.get(\"uddg\")\n",
    "            if uddg:\n",
    "                return urllib.parse.unquote(uddg[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5645514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query, max_results=SEARCH_RESULTS):\n",
    "    urls = []\n",
    "    seen = set()\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, max_results=max_results):\n",
    "            url = r.get(\"href\") or r.get(\"url\")\n",
    "            if not url:\n",
    "                continue\n",
    "            url = unwrap_ddg(url)\n",
    "            if url not in seen:\n",
    "                seen.add(url)\n",
    "                urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf7164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_text(url, timeout=TIMEOUT):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (research-agent)\"}\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)\n",
    "        if r.status_code != 200:\n",
    "            return \"\"\n",
    "        ct = r.headers.get(\"content-type\", \"\")\n",
    "        if \"html\" not in ct.lower():\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"svg\", \"iframe\", \"nav\", \"aside\"]):\n",
    "            tag.extract()\n",
    "        paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "        text = \" \".join([p for p in paragraphs if p])\n",
    "        if text.strip():\n",
    "            return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        meta = soup.find(\"meta\", attrs={\"name\": \"description\"}) or soup.find(\"meta\", attrs={\"property\": \"og:description\"})\n",
    "        if meta and meta.get(\"content\"):\n",
    "            return meta[\"content\"].strip()\n",
    "        if soup.title and soup.title.string:\n",
    "            return soup.title.string.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528f3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_passages(text, max_words=120):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunks.append(\" \".join(words[i:i+max_words]))\n",
    "    return chunks\n",
    "\n",
    "def split_sentences(text):\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [p.strip() for p in parts if p.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e181a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agent Class ---\n",
    "\n",
    "class ShortResearchAgent:\n",
    "    def __init__(self, embed_model=EMBEDDING_MODEL):\n",
    "        print(f\"Loading embedder: {embed_model} ...\")\n",
    "        self.embedder = SentenceTransformer(embed_model)\n",
    "\n",
    "    def run(self, query):\n",
    "        start = time.time()\n",
    "        summary = \"\"  # Initialize to avoid KeyError\n",
    "\n",
    "        # 1. Search\n",
    "        urls = search_web(query)\n",
    "\n",
    "        # 2. Fetch & Chunk\n",
    "        docs = []\n",
    "        for u in urls:\n",
    "            txt = fetch_text(u)\n",
    "            if not txt:\n",
    "                continue\n",
    "            chunks = chunk_passages(txt)\n",
    "            for c in chunks[:PASSAGES_PER_PAGE]:\n",
    "                docs.append({\"url\": u, \"passage\": c})\n",
    "\n",
    "        if not docs:\n",
    "            elapsed = round(time.time() - start, 2)\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"passages\": [],\n",
    "                \"summary\": \"No relevant documents found.\",\n",
    "                \"time\": elapsed,\n",
    "            }\n",
    "\n",
    "        # 3. Embed & Rank\n",
    "        texts = [d[\"passage\"] for d in docs]\n",
    "        emb_texts = self.embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        q_emb = self.embedder.encode(query, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "        sims = np.dot(emb_texts, q_emb)\n",
    "        top_idx = np.argsort(sims)[::-1][:TOP_PASSAGES]\n",
    "\n",
    "        top_passages = [\n",
    "            {\"url\": docs[i][\"url\"], \"passage\": docs[i][\"passage\"], \"score\": float(sims[i])}\n",
    "            for i in top_idx\n",
    "        ]\n",
    "\n",
    "        # 4. Summarize extractively\n",
    "        sentences = []\n",
    "        for tp in top_passages:\n",
    "            for s in split_sentences(tp[\"passage\"]):\n",
    "                if len(s.split()) < 6:\n",
    "                    continue\n",
    "                sentences.append({\"sent\": s, \"url\": tp[\"url\"]})\n",
    "\n",
    "        if sentences:\n",
    "            sent_texts = [s[\"sent\"] for s in sentences]\n",
    "            sent_embs = self.embedder.encode(sent_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "            sent_sims = np.dot(sent_embs, q_emb)\n",
    "            top_sent_idx = np.argsort(sent_sims)[::-1][:SUMMARY_SENTENCES]\n",
    "            chosen = [sentences[i] for i in top_sent_idx]\n",
    "\n",
    "            seen = set()\n",
    "            lines = []\n",
    "            for s in chosen:\n",
    "                key = s[\"sent\"].lower()[:80]\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                lines.append(f\"{s['sent']} (Source: {s['url']})\")\n",
    "            summary = \" \".join(lines)\n",
    "        else:\n",
    "            summary = \"No summary could be generated.\"\n",
    "\n",
    "        elapsed = round(time.time() - start, 2)\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"passages\": top_passages,\n",
    "            \"summary\": summary,\n",
    "            \"time\": elapsed,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92beb38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedder: sentence-transformers/all-MiniLM-L6-v2 ...\n",
      "Running query: What causes urban heat islands and how can cities reduce them?\n",
      "\n",
      "\n",
      "Top passages:\n",
      "- score 0.875 src https://evytor.vercel.app/blogs/urban-heat-islands-why-cities-feel-hotter\n",
      "  phenomenon has far-reaching implications, impacting everything from energy consumption and air quality to human health and ecosystem balance. Let's explore the science, impact, and solutions to this h...\n",
      "\n",
      "- score 0.823 src https://www.siradel.com/urban-heat-island-effect-causes-and-solutions/\n",
      "  way for cooler, greener and more resilient urban environments. Summary Understanding Urban Heat Islands What strategies can be used to mitigate urban heat islands effect? How to assess a territory and...\n",
      "\n",
      "- score 0.822 src https://evytor.vercel.app/blogs/urban-heat-islands-why-cities-feel-hotter\n",
      "  this: trees are nature's air conditioners! Cars, air conditioners, factories â€“ all these release heat into the environment. This \"waste heat\" contributes to the overall warming of urban areas. It's li...\n",
      "\n",
      "- score 0.817 src https://evytor.vercel.app/blogs/urban-heat-islands-why-cities-feel-hotter\n",
      "  Ever walked out of an air-conditioned building in a city and felt like you've hit a wall of heat? That's likely the urban heat island effect in action. Cities, with their concrete, asphalt, and lack o...\n",
      "\n",
      "- score 0.811 src https://www.seasidesustainability.org/post/urban-heat-islands-why-our-cities-are-heating-up-and-how-we-can-cool-them-down\n",
      "  Emma Wang Walk through any major city on a summer afternoon, and the temperature feels unmistakably higher than the climate in towns and forests just outside the borders. This isnâ€™t your imagination; ...\n",
      "\n",
      "--- Extractive summary ---\n",
      "ðŸ‘‡ So, what exactly causes urban heat islands? (Source: https://evytor.vercel.app/blogs/urban-heat-islands-why-cities-feel-hotter) Summary Understanding Urban Heat Islands What strategies can be used to mitigate urban heat islands effect? (Source: https://www.siradel.com/urban-heat-island-effect-causes-and-solutions/) This article dives into the science behind urban heat islands, exploring the causes, impacts, and potential solutions to mitigate this growing problem. (Source: https://evytor.vercel.app/blogs/urban-heat-islands-why-cities-feel-hotter)\n",
      "--------------------------\n",
      "\n",
      "Done in 30.6s\n"
     ]
    }
   ],
   "source": [
    "# --- Run Example ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = ShortResearchAgent()\n",
    "    query = \"What causes urban heat islands and how can cities reduce them?\"\n",
    "    print(f\"Running query: {query}\\n\")\n",
    "    out = agent.run(query)\n",
    "\n",
    "    print(\"\\nTop passages:\")\n",
    "    for p in out[\"passages\"]:\n",
    "        print(f\"- score {p['score']:.3f} src {p['url']}\\n  {p['passage'][:200]}...\\n\")\n",
    "\n",
    "    print(\"--- Extractive summary ---\")\n",
    "    print(out[\"summary\"])\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"\\nDone in {out['time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded0901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
